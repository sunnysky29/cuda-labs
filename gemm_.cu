// File: gemm_compare.cu
// CUDA Matrix Multiplication: Naive vs Tiled with Shared Memory
// Compile: nvcc -O3 -arch=sm_75 gemm_compare.cu -o gemm_compare
// Run: ./gemm_compare
//  çŸ©é˜µä¹˜æ³• share mem ä¼˜åŒ–

#include <cuda_runtime.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <iostream>
#include <chrono>

// ========================
// é…ç½®å‚æ•°
// ========================
#define N 512           // çŸ©é˜µå¤§å° (N x N)ï¼Œå¯æ”¹ä¸º 1024 æµ‹è¯•
#define TILE_M 16       // Shared memory tile size in M
#define TILE_N 16       // Shared memory tile size in N
#define TILE_K 8        // Shared memory tile size in K


// ========================
// CPU ç‰ˆæœ¬çŸ©é˜µä¹˜æ³•ï¼ˆé»„é‡‘æ ‡å‡†ï¼‰
// C = A * B, å…¶ä¸­ A, B, C éƒ½æ˜¯ N x N çŸ©é˜µï¼Œè¡Œä¼˜å…ˆå­˜å‚¨
// ========================
void matmul_cpu(const float *A, const float *B, float *C, int N_) {
    for (int i = 0; i < N_; ++i) {
        for (int j = 0; j < N_; ++j) {
            float sum = 0.0f;
            for (int k = 0; k < N_; ++k) {
                sum += A[i * N_ + k] * B[k * N_ + j];
            }
            C[i * N_ + j] = sum;
        }
    }
}

// ========================
// æœ´ç´ ç‰ˆæœ¬ Kernel
// ========================
__global__ void matmul_naive(const float *A, const float *B, float *C, int size) {
    // ------------------> x 
    // |
    // |
    // |
    // |
    // |
    // |
    // |
    //\|/
    // y
    int row = blockIdx.y * blockDim.y + threadIdx.y;  // yæ–¹å‘ -> è¡Œç´¢å¼•
    int col = blockIdx.x * blockDim.x + threadIdx.x;  // xæ–¹å‘ -> åˆ—ç´¢å¼•

    if (row < size && col < size) {
        float c = 0.0f;
        for (int k = 0; k < size; ++k) {
            c += A[row * size + k] * B[k * size + col];  // è¡Œä¼˜å…ˆè®¿é—®,/ å…¨å±€å†…å­˜è¯»å–
        }
        C[row * size + col] = c;
    }
}

// ========================
// ä¼˜åŒ–ç‰ˆæœ¬ Kernel: Tiled + Shared Memory
// æ¯ä¸ªçº¿ç¨‹å—å¤„ç† TILE_M x TILE_Nï¼Œä½¿ç”¨ shared memory
// ========================
__global__ void matmul_tiled1(const float *A, const float *B, float *C, int size) {
    // Shared memory tiles
    __shared__ float As[TILE_M][TILE_K];  // ç¼“å­˜ A çš„ä¸€å—: [TILE_M è¡Œ] Ã— [TILE_K åˆ—]
    __shared__ float Bs[TILE_K][TILE_N];  // ç¼“å­˜ B çš„ä¸€å—: [TILE_K è¡Œ] Ã— [TILE_N åˆ—]

    // === 1. çº¿ç¨‹èº«ä»½è¯†åˆ« ===
    int tx = threadIdx.x;        // å—å†…åˆ—ç´¢å¼• (0 ~ TILE_N-1)
    int ty = threadIdx.y;        // å—å†…è¡Œç´¢å¼• (0 ~ TILE_M-1)

    int block_col = blockIdx.x * TILE_N;   // å½“å‰ block è´Ÿè´£çš„èµ·å§‹åˆ—
    int block_row = blockIdx.y * TILE_M;   // å½“å‰ block è´Ÿè´£çš„èµ·å§‹è¡Œ

    int global_row = block_row + ty;       // å½“å‰çº¿ç¨‹è®¡ç®—çš„å…¨å±€è¡Œå·
    int global_col = block_col + tx;       // å½“å‰çº¿ç¨‹è®¡ç®—çš„å…¨å±€åˆ—å·

    float sum = 0.0f;  // ç´¯åŠ å™¨ï¼Œç”¨äºè®¡ç®— C[global_row][global_col]

    // === 2. åˆ†æ®µå¤„ç† K ç»´åº¦ï¼ˆk_start æ˜¯æ¯æ¬¡åŠ è½½çš„èµ·å§‹ kï¼‰===
    for (int k_start = 0; k_start < size; k_start += TILE_K) {

        // k_global > size å‘ç”Ÿåœ¨ çŸ©é˜µå¤§å° N ä¸èƒ½è¢« TILE_K æ•´é™¤ çš„æƒ…å†µä¸‹ï¼Œ
        // å°¤å…¶æ˜¯åœ¨æœ€åä¸€å—ï¼ˆlast tileï¼‰ åŠ è½½æ—¶ã€‚ 
        
        // --- åŠ è½½ A[global_row][k_start : k_start + TILE_K] åˆ° shared memory ---
        if (ty < TILE_M && tx < TILE_K && global_row < size) {
            int k_global = k_start + tx;  // å…¨å±€ k ç´¢å¼•
            As[ty][tx] = (k_global < size) ? A[global_row * size + k_global] : 0.0f;
        }

        // --- åŠ è½½ B[k_start : k_start + TILE_K][global_col] åˆ° shared memory ---
        if (tx < TILE_N && ty < TILE_K && global_col < size) {
            int k_global = k_start + ty;  // å…¨å±€ k ç´¢å¼•
            Bs[ty][tx] = (k_global < size) ? B[k_global * size + global_col] : 0.0f;
        }

        __syncthreads(); // === åŒæ­¥ï¼šç¡®ä¿æ‰€æœ‰çº¿ç¨‹å®ŒæˆåŠ è½½ ===

        // === è®¡ç®—éƒ¨åˆ†å’Œï¼šä½¿ç”¨å½“å‰ tile çš„ As å’Œ Bs ===
        #pragma unroll  // å‘Šè¯‰ç¼–è¯‘å™¨ï¼šæŠŠè¿™ä¸ªå¾ªç¯â€œå±•å¼€â€ï¼ˆunrollï¼‰ï¼Œä»¥å‡å°‘å¾ªç¯å¼€é”€ã€æé«˜æ€§èƒ½ã€‚ 
        for (int k_local = 0; k_local < TILE_K; ++k_local) {
            sum += As[ty][k_local] * Bs[k_local][tx];
        }

        // === åŒæ­¥ï¼šä¸ºä¸‹ä¸€è½®åŠ è½½åšå‡†å¤‡ï¼ˆé˜²æ­¢æ•°æ®ç«äº‰ï¼‰===
        __syncthreads();
    }

    // === 3. å†™å›ç»“æœ ===
    if (global_row < size && global_col < size) {
        C[global_row * size + global_col] = sum;
    }
}



// ========================
// åˆå§‹åŒ–çŸ©é˜µ
// ========================
void init_matrix(float *h_A, int size) {
    for (int i = 0; i < size * size; i++) {
        h_A[i] = rand() / (float)RAND_MAX;
    }

    printf("\n--- First 5x5 block of host matrix ---\n");
    for (int i = 0; i < 5; i++) {
        for (int j = 0; j < 5; j++) {
            printf("%8.3f ", h_A[i * N + j]);
        }
        printf("\n");
    }
}

// ========================
// CUDA é”™è¯¯æ£€æŸ¥å®
// ========================
#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA error at %s:%d - %s\n", __FILE__, __LINE__, cudaGetErrorString(err)); \
            exit(1); \
        } \
    } while(0)

// ========================
// ä¸»å‡½æ•°
// ========================
int main() {
    float *h_A, *h_B, *h_C_naive, *h_C_tiled;
    float *d_A, *d_B, *d_C_naive, *d_C_tiled;

    size_t bytes = N * N * sizeof(float);

    // ä¸»æœºå†…å­˜åˆ†é…
    h_A         = (float*)malloc(bytes);
    h_B         = (float*)malloc(bytes);
    h_C_naive   = (float*)malloc(bytes);
    h_C_tiled   = (float*)malloc(bytes);

    // åˆå§‹åŒ–
    srand(123);
    init_matrix(h_A, N);
    init_matrix(h_B, N);

    // è®¾å¤‡å†…å­˜åˆ†é…
    CUDA_CHECK(cudaMalloc(&d_A, bytes));
    CUDA_CHECK(cudaMalloc(&d_B, bytes));
    CUDA_CHECK(cudaMalloc(&d_C_naive, bytes));
    CUDA_CHECK(cudaMalloc(&d_C_tiled, bytes));

    // æ‹·è´æ•°æ®åˆ° GPU
    CUDA_CHECK(cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice));

    // å®šä¹‰çº¿ç¨‹å—å’Œç½‘æ ¼
    dim3 block(TILE_M, TILE_N);  // (x,y)
    dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);
    // ğŸ‘‰ æ‰“å°ç»´åº¦ä¿¡æ¯
    printf("\n=== CUDA Execution Configuration ===\n");
    printf("Matrix size        : %d x %d\n", N, N);
    printf("Tile size (M, N, K): (%d, %d, %d)\n", TILE_M, TILE_N, TILE_K);
    printf("Thread block       : (%d, %d)\n", block.x, block.y);
    printf("Grid size          : (%d, %d)\n", grid.x, grid.y);
    printf("Total blocks       : %d\n", grid.x * grid.y);
    printf("Threads per block  : %d\n", block.x * block.y);
    printf("Total threads      : %d\n", grid.x * grid.y * block.x * block.y);
    printf("====================================\n\n");

    // åˆ›å»º CUDA event ç”¨äºç²¾ç¡®è®¡æ—¶
    cudaEvent_t start, stop;
    CUDA_CHECK(cudaEventCreate(&start));
    CUDA_CHECK(cudaEventCreate(&stop));

    // ------------------- æµ‹é‡æœ´ç´ ç‰ˆæœ¬ -------------------
    CUDA_CHECK(cudaEventRecord(start));
    matmul_naive<<<grid, block>>>(d_A, d_B, d_C_naive, N);
    CUDA_CHECK(cudaEventRecord(stop));
    CUDA_CHECK(cudaEventSynchronize(stop));
    float time_naive;
    CUDA_CHECK(cudaEventElapsedTime(&time_naive, start, stop));

    // ------------------- æµ‹é‡ä¼˜åŒ–ç‰ˆæœ¬ -------------------
    CUDA_CHECK(cudaEventRecord(start));
    matmul_tiled1<<<grid, block>>>(d_A, d_B, d_C_tiled, N);
    CUDA_CHECK(cudaEventRecord(stop));
    CUDA_CHECK(cudaEventSynchronize(stop));
    float time_tiled;
    CUDA_CHECK(cudaEventElapsedTime(&time_tiled, start, stop));

    // æ‹·è´ç»“æœå› CPU
    CUDA_CHECK(cudaMemcpy(h_C_naive, d_C_naive, bytes, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaMemcpy(h_C_tiled, d_C_tiled, bytes, cudaMemcpyDeviceToHost));


    // ç®€å•éªŒè¯ï¼šæ£€æŸ¥æ‰€æœ‰å…ƒç´ æ˜¯å¦æ¥è¿‘
    bool pass = true;
    int first_error = -1;
    for (int i = 0; i < N * N; i++) {
        float diff = fabs(h_C_naive[i] - h_C_tiled[i]);
        if (diff > 1e-4) {
            pass = false;
            first_error = i;
            break;
        }
    }


    // ========== æ‰“å°çŸ©é˜µè°ƒè¯•ä¿¡æ¯ ==========
    if (!pass && first_error != -1) {
        int row = first_error / N;
        int col = first_error % N;
        printf("\n--- ERROR DETECTED ---\n");
        printf("First mismatch at C[%d][%d] (index %d):\n", row, col, first_error);
        printf("  Naive  = %.6f\n", h_C_naive[first_error]);
        printf("  Tiled  = %.6f\n", h_C_tiled[first_error]);
        printf("  Diff   = %.6f\n", fabs(h_C_naive[first_error] - h_C_tiled[first_error]));
    }

    // ========== æ‰“å°å‰ 5x5 å­çŸ©é˜µå¯¹æ¯” ==========
    printf("\n--- First 5x5 block of C (Naive) ---\n");
    for (int i = 0; i < 5; i++) {
        for (int j = 0; j < 5; j++) {
            printf("%8.3f ", h_C_naive[i * N + j]);
        }
        printf("\n");
    }

    printf("\n--- First 5x5 block of C (Tiled) ---\n");
    for (int i = 0; i < 5; i++) {
        for (int j = 0; j < 5; j++) {
            printf("%8.3f ", h_C_tiled[i * N + j]);
        }
        printf("\n");
    }

    // ------------------- CPU è®¡ç®—å‚è€ƒç»“æœ -------------------
    float *h_C_cpu = (float*)malloc(bytes);
    matmul_cpu(h_A, h_B, h_C_cpu, N);
    printf("\n--- First 5x5 block of C (CPU Reference) ---\n");
    for (int i = 0; i < 5; i++) {
        for (int j = 0; j < 5; j++) {
            printf("%8.3f ", h_C_cpu[i * N + j]);
        }
        printf("\n");
    }

    // ========== æ‰“å°å‡ºé”™ä½ç½®é™„è¿‘çš„ä¸Šä¸‹æ–‡ï¼ˆ3x3 åŒºåŸŸï¼‰==========
    if (!pass && first_error != -1) {
        int row = first_error / N;
        int col = first_error % N;
        int start_i = (row > 1) ? row - 1 : 0;
        int start_j = (col > 1) ? col - 1 : 0;
        int end_i = (row < N-1) ? row + 2 : N;
        int end_j = (col < N-1) ? col + 2 : N;

        printf("\n--- Context around mismatch C[%d][%d] ---\n", row, col);
        printf("         ");
        for (int j = start_j; j < end_j; j++) printf("  Naive[%d]   ", j);
        printf("   |    ");
        for (int j = start_j; j < end_j; j++) printf("  Tiled[%d]   ", j);
        printf("\n");

        for (int i = start_i; i < end_i; i++) {
            printf("Row %2d: ", i);
            for (int j = start_j; j < end_j; j++) {
                printf("%9.4f ", h_C_naive[i * N + j]);
            }
            printf("   |   ");
            for (int j = start_j; j < end_j; j++) {
                printf("%9.4f ", h_C_tiled[i * N + j]);
            }
            printf("\n");
        }
    }
    
    // è¾“å‡ºæ€§èƒ½
    printf("Matrix Size: %d x %d\n", N, N);
    printf("Naive Version Time: %.4f ms\n", time_naive);
    printf("Tiled  Version Time: %.4f ms\n", time_tiled);
    printf("Speedup: %.2fx\n", time_naive / time_tiled);
    printf("Correctness: %s\n", pass ? "PASS âˆš" : "FAIL Ã—");

    // é‡Šæ”¾èµ„æº
    free(h_A); free(h_B); free(h_C_naive); free(h_C_tiled);  free(h_C_cpu);  // æ–°å¢ï¼
    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C_naive); cudaFree(d_C_tiled);
    cudaEventDestroy(start); cudaEventDestroy(stop);

    return 0;
}